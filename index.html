<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="description" content="Simple project page template for your research paper, built with Astro and Tailwind CSS"><meta name="viewport" content="width=device-width"><meta name="generator" content="Astro v5.1.6"><link rel="icon" type="image/svg+xml" href="/favicon.svg"><meta property="og:title" content="HumanMM :Global Human Motion Recovery from Multi-shot Videos"><meta property="og:description" content="Simple project page template for your research paper, built with Astro and Tailwind CSS"><meta property="og:type" content="website"><meta property="og:image" content="/screenshot.png"><link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link href="https://fonts.googleapis.com/css2?family=Noto+Sans:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.css" integrity="sha384-WsHMgfkABRyG494OmuiNmkAOk8nhO1qE+Y6wns6v+EoNoTNxrWxYpl5ZYWFOLPCM" crossorigin="anonymous"><title>HumanMM :Global Human Motion Recovery from Multi-shot Videos</title><link rel="stylesheet" href="/_astro/index.CrZcQ-lg.css"></head> <body class="flex flex-col gap-4 items-center pt-12 pb-6 w-full *:px-6 *:max-w-[60rem]"> <header class="flex flex-col gap-10 items-center mb-6"> <h1>HumanMM :Global Human Motion Recovery from Multi-shot Videos</h1> <div class="flex flex-col gap-6 items-center"> <div class="flex flex-row gap-x-8 gap-y-4 flex-wrap justify-center"> <div class="flex flex-col items-center text-center"> <span class="text-xl flex flex-row"> <a href="https://scholar.google.com/citations?user=oV7sxpYAAAAJ&hl=zh-CN" class=""> Yuhong Zhang </a> <sup class="text-xl"> * </sup> </span> <p>Tsinghua University</p> </div><div class="flex flex-col items-center text-center"> <span class="text-xl flex flex-row"> <a href="https://guanlinwu123.github.io" class=""> Guanlin Wu </a> <sup class="text-xl"> * </sup> </span> <p>Johns Hopkins University</p> </div><div class="flex flex-col items-center text-center"> <span class="text-xl flex flex-row"> <a href="https://lhchen.top/" class=""> Linghao Chen </a>  </span> <p>Tsinghua University</p> </div><div class="flex flex-col items-center text-center"> <span class="text-xl flex flex-row"> <a href="https://zhuokai-zhao.com/" class=""> Zhuokai Zhao </a>  </span> <p>Meta</p> </div><div class="flex flex-col items-center text-center"> <span class="text-xl flex flex-row"> <a href="https://jinglin7.github.io/" class=""> Jing Lin </a>  </span> <p>Tsinghua University</p> </div><div class="flex flex-col items-center text-center"> <span class="text-xl flex flex-row"> Xiaoke Jiang  </span> <p>IDEA</p> </div><div class="flex flex-col items-center text-center"> <span class="text-xl flex flex-row"> Jiamin Wu  </span> <p>IDEA</p> </div><div class="flex flex-col items-center text-center"> <span class="text-xl flex flex-row"> Zhuoheng Li  </span> <p>HKU</p> </div><div class="flex flex-col items-center text-center"> <span class="text-xl flex flex-row"> Hao Frank Yang  </span> <p>Johns Hopkins University</p> </div><div class="flex flex-col items-center text-center"> <span class="text-xl flex flex-row"> <a href="https://www.sigs.tsinghua.edu.cn/whq_en/main.htm" class=""> Haoqian Wang </a> <sup class="text-xl"> † </sup> </span> <p>Tsinghua University</p> </div><div class="flex flex-col items-center text-center"> <span class="text-xl flex flex-row"> <a href="https://www.leizhang.org/" class=""> Lei Zhang </a> <sup class="text-xl"> † </sup> </span> <p>IDEA</p> </div> </div> <p class="text-center">CVPR 2025</p> <p class="text-sm text-center">  <sup>*</sup>Equal Contribution, <sup>†</sup>Corresponding Author </p> <div class="flex flex-row flex-wrap justify-center gap-2"> <a href="https://openreview.net/group?id=thecvf.com/CVPR/2025/Conference/Authors&referrer=%5BHomepage%5D(%2F)" class="flex flex-row bg-zinc-800 text-white rounded-full gap-2 items-center text-lg px-5 py-2 hover:bg-zinc-900 hover:no-underline"> <svg width="1em" height="1em" data-icon="ri:file-pdf-2-line">   <symbol id="ai:ri:file-pdf-2-line" viewBox="0 0 24 24"><path fill="currentColor" d="M5 4h10v4h4v12H5zM3.999 2A.995.995 0 0 0 3 2.992v18.016a1 1 0 0 0 .993.992h16.014A1 1 0 0 0 21 20.992V7l-5-5zm6.5 5.5c0 1.577-.455 3.437-1.224 5.153c-.772 1.723-1.814 3.197-2.9 4.066l1.18 1.613c2.927-1.952 6.168-3.29 9.304-2.842l.457-1.939C14.644 12.661 12.5 9.99 12.5 7.5zm.6 5.972c.268-.597.505-1.216.705-1.843a9.7 9.7 0 0 0 1.706 1.966c-.982.176-1.944.465-2.875.833q.248-.471.465-.956"/></symbol><use href="#ai:ri:file-pdf-2-line"></use>  </svg> <span>Paper</span> </a><a href="https://github.com/zhangyuhong01/HumanMM" class="flex flex-row bg-zinc-800 text-white rounded-full gap-2 items-center text-lg px-5 py-2 hover:bg-zinc-900 hover:no-underline"> <svg width="1em" height="1em" data-icon="ri:github-line">   <symbol id="ai:ri:github-line" viewBox="0 0 24 24"><path fill="currentColor" d="M5.884 18.653c-.3-.2-.558-.455-.86-.816a51 51 0 0 1-.466-.579c-.463-.575-.755-.841-1.056-.95a1 1 0 1 1 .675-1.882c.752.27 1.261.735 1.947 1.588c-.094-.117.34.427.433.539c.19.227.33.365.44.438c.204.137.588.196 1.15.14c.024-.382.094-.753.202-1.095c-2.968-.726-4.648-2.64-4.648-6.396c0-1.24.37-2.356 1.058-3.292c-.218-.894-.185-1.975.302-3.192a1 1 0 0 1 .63-.582c.081-.024.127-.035.208-.047c.803-.124 1.937.17 3.415 1.096a11.7 11.7 0 0 1 2.687-.308c.912 0 1.819.104 2.684.308c1.477-.933 2.614-1.227 3.422-1.096q.128.02.218.05a1 1 0 0 1 .616.58c.487 1.216.52 2.296.302 3.19c.691.936 1.058 2.045 1.058 3.293c0 3.757-1.674 5.665-4.642 6.392c.125.415.19.878.19 1.38c0 .665-.002 1.299-.007 2.01c0 .19-.002.394-.005.706a1 1 0 0 1-.018 1.958c-1.14.227-1.984-.532-1.984-1.525l.002-.447l.005-.705c.005-.707.008-1.337.008-1.997c0-.697-.184-1.152-.426-1.361c-.661-.57-.326-1.654.541-1.751c2.966-.333 4.336-1.482 4.336-4.66c0-.955-.312-1.744-.913-2.404A1 1 0 0 1 17.2 6.19c.166-.414.236-.957.095-1.614l-.01.003c-.491.139-1.11.44-1.858.949a1 1 0 0 1-.833.135a9.6 9.6 0 0 0-2.592-.349c-.89 0-1.772.118-2.592.35a1 1 0 0 1-.829-.134c-.753-.507-1.374-.807-1.87-.947c-.143.653-.072 1.194.093 1.607a1 1 0 0 1-.189 1.045c-.597.655-.913 1.458-.913 2.404c0 3.172 1.371 4.328 4.322 4.66c.865.097 1.202 1.177.545 1.748c-.193.168-.43.732-.43 1.364v3.15c0 .985-.834 1.725-1.96 1.528a1 1 0 0 1-.04-1.962v-.99c-.91.061-1.661-.088-2.254-.485"/></symbol><use href="#ai:ri:github-line"></use>  </svg> <span>Code</span> </a><a href class="flex flex-row bg-zinc-800 text-white rounded-full gap-2 items-center text-lg px-5 py-2 hover:bg-zinc-900 hover:no-underline"> <svg width="0.88em" height="1em" data-icon="academicons:arxiv">   <symbol id="ai:academicons:arxiv" viewBox="0 0 448 512"><path fill="currentColor" d="M62.258 8.006a22.22 22.22 0 0 0-20.929 13.448c-3.404 8.169-.96 13.898 6.506 24.59c10.935 16.09 122.178 149.673 122.178 149.673l-24.619 23.038c-20.74 20.735-21.632 48.566-2.34 67.852l28.663 27.3l-79.976 98.235c-6.21 6.614-10.053 18.221-6.585 26.552a22.7 22.7 0 0 0 21.21 14.06a20.23 20.23 0 0 0 15.249-7.536l95.122-88.437L363.33 496.39a27.14 27.14 0 0 0 18.418 7.61a25.3 25.3 0 0 0 7.335-1.108a27.66 27.66 0 0 0 18.4-18.99a25.6 25.6 0 0 0-6.481-23.69L272.219 305.195l23.062-21.443c17.198-15.504 17.29-42.455.197-58.076l-25.257-24.228L357.417 98.46l.115-.133l.103-.14c7.793-10.123 11.52-17.92 7.502-27.806a36.17 36.17 0 0 0-23.647-18.37a24 24 0 0 0-3.166-.212l-.006.018a28.52 28.52 0 0 0-18.252 8.123l-.203.166l-.19.173L218.6 151.925L79.261 18.253S70.995 8.213 62.258 8.006m276.06 51.214q1.115.004 2.22.148a29.3 29.3 0 0 1 17.719 13.81c2.246 5.523 1.554 10.01-6.506 20.484L264.861 196.3l-40.882-39.22l100.68-91.304a21.77 21.77 0 0 1 13.66-6.536zM175.077 201.127L395.19 464.872c4.32 5.408 7.02 10.818 5.18 16.914a20.25 20.25 0 0 1-13.463 14.037a17.6 17.6 0 0 1-5.17.784a19.8 19.8 0 0 1-13.293-5.56l-220.15-209.694c-17.317-17.316-14.698-40.33 2.158-57.186z"/></symbol><use href="#ai:academicons:arxiv"></use>  </svg> <span>arXiv</span> </a><a href class="flex flex-row bg-zinc-800 text-white rounded-full gap-2 items-center text-lg px-5 py-2 hover:bg-zinc-900 hover:no-underline"> <svg width="0.88em" height="1em" viewBox="0 0 448 512" data-icon="academicons:arxiv">   <use href="#ai:academicons:arxiv"></use>  </svg> <span>Dataset</span> </a> </div> </div> </header>
<h2 id="demo-video">Demo Video</h2>
<div class="w-full flex justify-center"> <video class="w-full h-auto aspect-video rounded-lg" autoplay controls muted loop> <source src="/_astro/outside.DlwaL9XQ.mp4" type="video/mp4"> </video> </div>
<figure class="w-full flex flex-col gap-2 items-center"> <div class="*:max-h-[35rem] *:object-contain flex justify-center w-full"> <img src="/_astro/teaser.CFATnmzQ_ZWBOc0.webp" alt="Pingpong." width="3073" height="1798" loading="lazy" decoding="async" class="rounded-lg max-h-[35rem] w-max object-contain !px-0"> </div> <figcaption class="text-center text-zinc-700"> Recovering a human motion from multi-shot videos. Top: We take two multi-shot table tennis game videos with shot transitions as input. We aim to recover two motions of two athletes (Long MA and Zhendong FAN) from two videos, respectively. The
first video is recorded by three shots (“①”, “②”, and “③” ), and the second one is recovered by two shots (“④” and “⑤” ). Bottom: We recover two motions (Long MA in green and Zhendong FAN in pink), different shots, and camera poses for each multi-shot video. The
recovered motion is aligned with the motion in the videos </figcaption> </figure>
<section class="!max-w-full !px-0 py-6 w-full mx-0 flex justify-center" style="background-color: #e4e4e7;"> <div class="flex flex-col gap-4 items-center w-full max-w-[60rem] [&>h2]:mt-0 px-6"> <h2 id="abstract">Abstract</h2><p>we present a novel framework designed to reconstruct long-sequence 3D human motion in the world coordinates from in-the-wild videos with multiple shot transitions.
Such long-sequence in-the-wild motions are highly valuable to applications such as motion generation and motion understanding, but are of great challenge to be
recovered due to abrupt shot transitions, partial occlusions, and dynamic backgrounds presented in such videos. Existing methods primarily focus on single-shot videos,
where continuity is maintained within a single camera view, or simplify multi-shot alignment in camera space only. In this work, we tackle the challenges by integrating an enhanced camera pose estimation with Human Motion Recovery (HMR) by incorporating a shot transition detector and a robust alignment module for accurate pose and orientation continuity across shots. By leveraging a custom motion integrator, we effectively mitigate the problem of foot sliding and ensure temporal consistency in human pose. Extensive evaluations on our created multi-shot dataset from public 3D human datasets demonstrate the robustness of our method in reconstructing realistic human motion in world coordinates.</p> </div> </section>
<h2 id="pipeline">Pipeline</h2>
<p>HumanMM processes multi-shot video sequences by first extracting motion feature such as keypoints and bounding boxes, using ViTPose and image feature using ViT.
These features are then segmented into single-shot clips via Shot Transition Detection (Sec. 3.2). Initialized camera (camera rotation R and camera translation T)
and human (SMPL) parameters for each shot are estimated using Masked LEAP-VO (Sec. 3.3) and GVHMR. Human orientation is aligned across shots through camera calibration
(3.4.1), and ms-HMR (Sec. 3.4.2) ensures consistent pose alignment. Finally, a bi-directional LSTM-based trajectory predictor with trajectory refiner predicts trajectory based on aligned motion and mitigates foot sliding throughout the video.</p>
<figure class="w-full flex flex-col gap-2 items-center"> <div class="*:max-h-[35rem] *:object-contain flex justify-center w-full"> <img src="/_astro/ms-pipeline.D8T4_hLd_zXa6r.webp" alt="Pingpong." width="3213" height="991" loading="lazy" decoding="async" class="rounded-lg max-h-[35rem] w-max object-contain !px-0"> </div> <figcaption class="text-center text-zinc-700"> Diagram of the transformer deep learning architecture. </figcaption> </figure>
<h2 id="core-modules">Core Modules</h2>
<p>Use the two columns component to display two columns of content. In this example, the first column contains a figure with a YouTube video and the second column contains a figure with a custom <a href="https://react.dev/">React</a> component. By default, they display side by side, but if the screen is narrow enough (for example, on mobile), they’re arranged vertically.</p>
<div class="flex flex-wrap gap-4 items-center w-full"> <div class="flex-1 min-w-[16rem]"> <figure class="w-full flex flex-col gap-2 items-center"> <div class="*:max-h-[35rem] *:object-contain flex justify-center w-full"> <img src="/_astro/mshmr.BMDK41SM_18ncbV.webp" alt="Pingpong." width="2047" height="1720" loading="lazy" decoding="async" class="rounded-lg max-h-[35rem] w-max object-contain !px-0"> </div> <figcaption class="text-center text-zinc-700"> MS-HMR </figcaption> </figure> </div> <div class="flex-1 min-w-[16rem]"> <figure class="w-full flex flex-col gap-2 items-center"> <div class="*:max-h-[35rem] *:object-contain flex justify-center w-full"> <img src="/_astro/OAM.CfOVcrtO_Z1ya9e3.webp" alt="Pingpong." width="3183" height="1393" loading="lazy" decoding="async" class="rounded-lg max-h-[35rem] w-max object-contain !px-0"> </div> <figcaption class="text-center text-zinc-700"> Orient Alignment Module </figcaption> </figure> </div> </div>
<h2 id="aligning-human-motion-between-shots">Aligning Human Motion Between Shots</h2>
<p>To align the orientations between two frames with shot transition, we decompose the human orientation with shot transitions in world
coordinates as,</p>
<div class="text-center"><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="monospace">R</mi><mo stretchy="false">(</mo><msub><mi mathvariant="normal">Γ</mi><mtext>world</mtext></msub><mo stretchy="false">)</mo><mo>=</mo><msub><mi mathvariant="bold">R</mi><msub><mi>δ</mi><mtext>cam</mtext></msub></msub><mi mathvariant="monospace">R</mi><mo stretchy="false">(</mo><msub><mi mathvariant="normal">Γ</mi><mtext>view</mtext></msub><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\mathtt{R}(\Gamma_{\text{world}}) = \mathbf{R}_{\delta_{\text{cam}}} \mathtt{R}(\Gamma_{\text{view}}),</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathtt">R</span><span class="mopen">(</span><span class="mord"><span class="mord">Γ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">world</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0001em;vertical-align:-0.2501em;"></span><span class="mord"><span class="mord mathbf">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03785em;">δ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1645em;"><span style="top:-2.357em;margin-left:-0.0379em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">cam</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2501em;"><span></span></span></span></span></span></span><span class="mord mathtt">R</span><span class="mopen">(</span><span class="mord"><span class="mord">Γ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">view</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mpunct">,</span></span></span></span></span></div>
<h2 id="experiment">Experiment</h2>
<p>We compare our proposed method with several state-of-the-art HMR methods SLAHMR, WHAM and GVHMR on our proposed benchmark. As illustrated in the table below,
our proposed method has achieved the best performance for PA&amp;WA-MPJPE, RTE and ROE through videos with all numbers of shots across \msaist and \msm, indicating
that our method reconstructs both the global human motion and orientations in the world coordinates more accurately and robustly. For the foot sliding metric,
our method also performs as the best on \msm across all numbers of shots.</p>
<figure class="w-full flex flex-col gap-2 items-center"> <div class="*:max-h-[35rem] *:object-contain flex justify-center w-full"> <img src="/_astro/main-experiment.gLEziV9P_Z1rtkKk.webp" alt="exp1." width="2700" height="757" loading="lazy" decoding="async" class="rounded-lg max-h-[35rem] w-max object-contain !px-0"> </div> <figcaption class="text-center text-zinc-700">  </figcaption> </figure>
<br/>
<br/>
<h2 id="visualization-of-comparison-between-existing-methods">Visualization of Comparison between Existing Methods</h2>
<figure class="w-full flex flex-col gap-2 items-center"> <div class="*:max-h-[35rem] *:object-contain flex justify-center w-full"> <img src="/_astro/vis_comp_1_00.B8lnvmBN_Z236ane.webp" alt="exp1." width="3647" height="2372" loading="lazy" decoding="async" class="rounded-lg max-h-[35rem] w-max object-contain !px-0"> </div> <figcaption class="text-center text-zinc-700">  </figcaption> </figure>
<figure class="w-full flex flex-col gap-2 items-center"> <div class="*:max-h-[35rem] *:object-contain flex justify-center w-full"> <img src="/_astro/vis_comp_2_00.DW3dVOAA_2hxhX7.webp" alt="exp1." width="2648" height="1750" loading="lazy" decoding="async" class="rounded-lg max-h-[35rem] w-max object-contain !px-0"> </div> <figcaption class="text-center text-zinc-700">  </figcaption> </figure>
<figure class="w-full flex flex-col gap-2 items-center"> <div class="*:max-h-[35rem] *:object-contain flex justify-center w-full"> <img src="/_astro/vis_comp_3_00.B2sVhT4-_265Xn.webp" alt="exp1." width="2972" height="1839" loading="lazy" decoding="async" class="rounded-lg max-h-[35rem] w-max object-contain !px-0"> </div> <figcaption class="text-center text-zinc-700">  </figcaption> </figure>
<br/>
<h2 id="visualization-of-in-the-wild-multi-shot-video">Visualization of in-the-wild multi-shot video</h2>
<br/>
<figure class="w-full flex flex-col gap-2 items-center"> <div class="*:max-h-[35rem] *:object-contain flex justify-center w-full"> <img src="/_astro/case1.BeSW40XN_1bfBnX.webp" alt="exp1." width="2015" height="1378" loading="lazy" decoding="async" class="rounded-lg max-h-[35rem] w-max object-contain !px-0"> </div> <figcaption class="text-center text-zinc-700">  </figcaption> </figure>
<figure class="w-full flex flex-col gap-2 items-center"> <div class="*:max-h-[35rem] *:object-contain flex justify-center w-full"> <img src="/_astro/case2.BOk4UeBD_Z1iBUcc.webp" alt="exp1." width="1788" height="1089" loading="lazy" decoding="async" class="rounded-lg max-h-[35rem] w-max object-contain !px-0"> </div> <figcaption class="text-center text-zinc-700">  </figcaption> </figure>
<h2 id="bibtex-citation">BibTeX citation</h2>
<div class="px-6 w-full"> <pre class="relative flex flex-col whitespace-pre overflow-auto bg-zinc-300 p-4 rounded-lg w-full">    <code><span class="line"><span style="color:#D73A49">@misc</span><span style="color:#24292E">{</span><span style="color:#6F42C1">roman2024academic</span><span style="color:#24292E">,</span></span>
<span class="line"><span style="color:#005CC5">  author</span><span style="color:#24292E"> = </span><span style="color:#032F62">&quot;</span><span style="color:#24292E">{Roman Hauksson}</span><span style="color:#032F62">&quot;</span><span style="color:#24292E">,</span></span>
<span class="line"><span style="color:#005CC5">  title</span><span style="color:#24292E"> = </span><span style="color:#032F62">&quot;</span><span style="color:#24292E">Academic Project Page Template</span><span style="color:#032F62">&quot;</span><span style="color:#24292E">,</span></span>
<span class="line"><span style="color:#005CC5">  year</span><span style="color:#24292E"> = </span><span style="color:#032F62">&quot;</span><span style="color:#24292E">2024</span><span style="color:#032F62">&quot;</span><span style="color:#24292E">,</span></span>
<span class="line"><span style="color:#005CC5">  howpublished</span><span style="color:#24292E"> = </span><span style="color:#032F62">&quot;</span><span style="color:#24292E">\url{https://research-template.roman.technology}</span><span style="color:#032F62">&quot;</span><span style="color:#24292E">,</span></span>
<span class="line"><span style="color:#24292E">}</span></span></code>
  </pre> </div> <footer class="m-auto"> <p class="text-zinc-500 text-sm">
This page was built using <a href="https://github.com/RomanHauksson/academic-project-astro-template">Roman Hauksson's academic project page template</a>, which was adapted from <a href="https://github.com/eliahuhorwitz/Academic-project-page-template">Eliahu Horwitz's template</a>, which was adapted from <a href="https://nerfies.github.io/">Keunhong Park's project page for <i>Nerfies</i></a>. It is licensed under a <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
</p> </footer> </body></html>